unique(SVM_validscale2$cat1)
unique(testData_scale2$cat2)
unique(SVM_validscale2$cat2)
unique(testData_scale2$cat3)
unique(SVM_validscale2$cat3)
unique(testData_scale2$cat4)
unique(SVM_validscale2$cat4)
unique(testData_scale2$cat5)
unique(SVM_validscale2$cat5)
test_pred=predict(svm1,testData_scale2) # NO clue why this is breaking. I've tried changing values to factors.
unique(testData_scale2$cat6)
unique(SVM_validscale2$cat6)
unique(testData_scale2$cat7)
unique(SVM_validscale2$cat7)
unique(testData_scale2$cat8)
unique(SVM_validscale2$cat8)
unique(testData_scale2$cat9)
unique(SVM_validscale2$cat9)
unique(testData_scale2$cat10)
unique(SVM_validscale2$cat10)
unique(testData_scale2$cat11)
unique(SVM_validscale2$cat11)
unique(testData_scale2$cat12)
unique(SVM_validscale2$cat12)
unique(testData_scale2$cat13)
unique(SVM_validscale2$cat13)
unique(testData_scale2$cat14)
unique(SVM_validscale2$cat14)
unique(testData_scale2$cat15)
unique(SVM_validscale2$cat15)
unique(testData_scale2$cat16)
unique(SVM_validscale2$cat16)
unique(testData_scale2$cat17)
unique(SVM_validscale2$cat17)
unique(testData_scale2$cat18)
unique(SVM_validscale2$cat18)
unique(testData_scale2$cat19)
unique(SVM_validscale2$cat19)
unique(testData_scale2$cat20)
unique(SVM_validscale2$cat20)
unique(testData_scale2$cat21)
unique(SVM_validscale2$cat21)
unique(testData_scale2$cat22)
unique(SVM_validscale2$cat22)
unique(testData_scale2$cat23)
unique(SVM_validscale2$cat23)
unique(testData_scale2$cat24)
unique(SVM_validscale2$cat24)
unique(testData_scale2$cat25)
unique(SVM_validscale2$cat25)
unique(testData_scale2$cat26)
unique(SVM_validscale2$cat26)
unique(testData_scale2$cat27)
accuracy_xgb=vector()
eta=seq(.05,1,by=.05)
depth = seq(1,15, by=1)
i=1
for (d in depth) {
for (e in eta) {
print(i)
xgb <- xgboost(data = sparse_train,
label = train_label,
eta = 0,
max_depth = 20,
gamma = 0,
nround=100,
subsample = 0.75,
colsample_bytree = 0.75,
objective = "reg:linear",
nthread = 3,
eval_metric = 'rmse',
verbose =0)
ptrain = predict(xgb, sparse_train)
pvalid = predict(xgb, sparse_valid)
abserror = abs(df_XG_validate$target - pvalid)
accuracy_xgb[i] = mean(abserror)
i=i+1
}
}
plot(eta, accuracy_xgb)
plot(depth, accuracy_xgb)
# Tune for eta
accuracy_xgb=vector()
eta=seq(.05,1,by=.05)
accuracy_xgb=vector()
eta=seq(.05,1,by=.05)
depth = seq(1,15, by=1)
i=1
for (d in depth) {
for (e in eta) {
print(i)
xgb <- xgboost(data = sparse_train,
label = train_label,
eta = 0,
max_depth = d,
gamma = 0,
nround=100,
subsample = 0.75,
colsample_bytree = 0.75,
objective = "reg:linear",
nthread = 3,
eval_metric = 'rmse',
verbose =0)
ptrain = predict(xgb, sparse_train)
pvalid = predict(xgb, sparse_valid)
abserror = abs(df_XG_validate$target - pvalid)
accuracy_xgb[i] = mean(abserror)
i=i+1
}
}
df_XG_train<-df_ML[intrain,]
df_XG_validate<-df_ML[-intrain,]
##############################################################################################################
#Changing all categorical vars to factors
df_XG_train[cat.col] = lapply(df_XG_train[cat.col], factor)
df_XG_validate[cat.col] = lapply(df_XG_validate[cat.col], factor)
sparse_train = sparse.model.matrix(target ~ . -target , data=df_XG_train)
sparse_valid = sparse.model.matrix(target  ~ . -target , data=df_XG_validate)
train_label = as.numeric(df_XG_train$target)[df_XG_train$target]
accuracy_xgb=vector()
eta=seq(.05,1,by=.05)
depth = seq(1,15, by=1)
i=1
for (d in depth) {
for (e in eta) {
print(i)
xgb <- xgboost(data = sparse_train,
label = train_label,
eta = 0,
max_depth = d,
gamma = 0,
nround=100,
subsample = 0.75,
colsample_bytree = 0.75,
objective = "reg:linear",
nthread = 3,
eval_metric = 'rmse',
verbose =0)
ptrain = predict(xgb, sparse_train)
pvalid = predict(xgb, sparse_valid)
abserror = abs(df_XG_validate$target - pvalid)
accuracy_xgb[i] = mean(abserror)
i=i+1
}
}
plot(eta, accuracy_xgb)
plot(eta, accuracy_xgb)
View(accuracy_xgb)
accuracy_xgb=vector()
eta=seq(.05,1,by=.05)
i=1
for (e in eta) {
print(i)
xgb <- xgboost(data = sparse_train,
label = train_label,
eta = e,
max_depth = 20,
gamma = 0,
nround=100,
subsample = 0.75,
colsample_bytree = 0.75,
objective = "reg:linear",
nthread = 3,
eval_metric = 'rmse',
verbose =0)
ptrain = predict(xgb, sparse_train)
pvalid = predict(xgb, sparse_valid)
abserror = abs(df_XG_validate$target - pvalid)
accuracy_xgb[i] = mean(abserror)
i=i+1
}
plot(eta, accuracy_xgb)
accuracy=vector()
mtry=seq(2,86,by=2)
i=1
for(m in mtry){
print(i)
rf = randomForest(target ~ ., data=df_ML_train,mtry=m, ntree=50)
test_pred =  predict(rf,df_ML_validate)
#accuracy[i] =  sum(test_pred!=df_ML_train$target)/nrow(df_ML_train)
abserror = abs(df_ML_validate$target - test_pred)
accuracy[i] = mean(abserror)
i=i+1
}
plot(mtry, accuracy)
accuracy=vector()
ntree=seq(25,200,by=25)
i=1
for(n in ntree){
print(i)
rf = randomForest(target ~ ., data=df_ML_train,mtry=22, ntree=n)
test_pred =  predict(rf,df_ML_validate)
#accuracy[i] =  sum(test_pred!=df_ML_train$target)/nrow(df_ML_train)
abserror = abs(df_ML_validate$target - test_pred)
accuracy[i] = mean(abserror)
i=i+1
}
plot(ntree, accuracy)
accuracy_xgb=vector()
eta=seq(.05,1,by=.05)
i=1
for (e in eta) {
print(i)
xgb <- xgboost(data = sparse_train,
label = train_label,
eta = e,
max_depth = 20,
gamma = 0,
nround=100,
subsample = 0.75,
colsample_bytree = 0.75,
objective = "reg:linear",
nthread = 3,
eval_metric = 'rmse',
verbose =0)
ptrain = predict(xgb, sparse_train)
pvalid = predict(xgb, sparse_valid)
abserror = abs(df_XG_validate$target - pvalid)
accuracy_xgb[i] = mean(abserror)
i=i+1
}
plot(eta, accuracy_xgb)
accuracy_xgb=vector()
depth = seq(1,15, by=1)
i=1
for (d in depth) {
print(i)
xgb <- xgboost(data = sparse_train,
label = train_label,
eta = 0.5,
max_depth = d,
gamma = 0,
nround=100,
subsample = 0.75,
colsample_bytree = 0.75,
objective = "reg:linear",
nthread = 3,
eval_metric = 'rmse',
verbose =0)
ptrain = predict(xgb, sparse_train)
pvalid = predict(xgb, sparse_valid)
abserror = abs(df_XG_validate$target - pvalid)
accuracy_xgb[i] = mean(abserror)
i=i+1
}
plot(depth, accuracy_xgb)
accuracy_xgb=vector()
gam = seq(0,1, by=.1)
i=1
for (g in gam) {
print(i)
xgb <- xgboost(data = sparse_train,
label = train_label,
eta = 0.5,
max_depth = 5,
gamma = g,
nround=100,
subsample = 0.75,
colsample_bytree = 0.75,
objective = "reg:linear",
nthread = 3,
eval_metric = 'rmse',
verbose =0)
ptrain = predict(xgb, sparse_train)
pvalid = predict(xgb, sparse_valid)
abserror = abs(df_XG_validate$target - pvalid)
accuracy_xgb[i] = mean(abserror)
i=i+1
}
plot(gam, accuracy_xgb)
xgb <- xgboost(data = sparse_train,
label = train_label,
eta = 0.05,
max_depth = 5,
gamma = 0.6,
nround=100,
subsample = 0.75,
colsample_bytree = 0.75,
objective = "reg:linear",
nthread = 3,
eval_metric = 'rmse',
verbose =0)
# Predict
ptrain = predict(xgb, sparse_train)
pvalid = predict(xgb, sparse_valid)
# MAE
df_XG_train$Prediction = ptrain
df_XG_train$abserror = abs(df_XG_train$target - df_XG_train$Prediction)
MAE = mean(df_XG_train$abserror)
print(MAE)
df_XG_validate$Prediction = pvalid
df_XG_validate$abserror = abs(df_XG_validate$target - df_XG_validate$Prediction)
MAE = mean(df_XG_validate$abserror)
print(MAE)
xgb <- xgboost(data = sparse_train,
label = train_label,
eta = 0.05,
max_depth = 10,
gamma = 0.1,
nround=100,
subsample = 0.75,
colsample_bytree = 0.75,
objective = "reg:linear",
nthread = 3,
eval_metric = 'rmse',
verbose =0)
# Predict
ptrain = predict(xgb, sparse_train)
pvalid = predict(xgb, sparse_valid)
# MAE
df_XG_train$Prediction = ptrain
df_XG_train$abserror = abs(df_XG_train$target - df_XG_train$Prediction)
MAE = mean(df_XG_train$abserror)
print(MAE)
df_XG_validate$Prediction = pvalid
df_XG_validate$abserror = abs(df_XG_validate$target - df_XG_validate$Prediction)
MAE = mean(df_XG_validate$abserror)
print(MAE)
xgb <- xgboost(data = sparse_train,
label = train_label,
eta = 0.05,
max_depth = 5,
gamma = 0.1,
nround=100,
subsample = 0.75,
colsample_bytree = 0.75,
objective = "reg:linear",
nthread = 3,
eval_metric = 'rmse',
verbose =0)
# Predict
ptrain = predict(xgb, sparse_train)
pvalid = predict(xgb, sparse_valid)
# MAE
df_XG_train$Prediction = ptrain
df_XG_train$abserror = abs(df_XG_train$target - df_XG_train$Prediction)
MAE = mean(df_XG_train$abserror)
print(MAE)
df_XG_validate$Prediction = pvalid
df_XG_validate$abserror = abs(df_XG_validate$target - df_XG_validate$Prediction)
MAE = mean(df_XG_validate$abserror)
print(MAE)
# Predict
test_pred=predict(svm1,testData_scale2) # NO clue why this is breaking.
# Predict on test
testData = read.csv('C:\\Users\\jlmic\\Documents\\Machine Learning\\Data\\testData.csv')
dim(testData)
df_ML = read.csv('C:\\Users\\jlmic\\Documents\\Machine Learning\\Data\\MLProjectData.csv')
#df_ML = read.csv('C:\\Users\\Steven\\Documents\\MSA\\Analytics Foundations\\Machine Learning\\data\\MLProjectData.csv')
#df_ML= read.csv('C:\\Users\\chels\\Desktop\\MSA\\Fall 3\\Machine Learning\\Project\\MLProjectData.csv')
dim(df_ML
#df_ML = read.csv('C:\\Users\\Steven\\Documents\\MSA\\Analytics Foundations\\Machine Learning\\data\\MLProjectData.csv')
#df_ML= read.csv('C:\\Users\\chels\\Desktop\\MSA\\Fall 3\\Machine Learning\\Project\\MLProjectData.csv')
dim(df_ML)
View(testData)
df_ML = read.csv('C:\\Users\\Steven\\Documents\\MSA\\Analytics Foundations\\Machine Learning\\data\\MLProjectData.csv')
rm(list=ls())
library(tidyverse)
library(caret)
options(digits=4)
df_ML = read.csv('C:\\Users\\Steven\\Documents\\MSA\\Analytics Foundations\\Machine Learning\\data\\MLProjectData.csv')
# column groups
cat.col = c('cat1','cat2','cat3','cat4','cat5','cat6','cat7','cat8','cat9','cat10','cat11','cat12','cat13','cat14','cat15',
'cat16','cat17','cat18','cat19','cat20','cat21','cat22','cat23','cat24','cat25','cat26')
num.col <- paste('num',seq(1:59), sep='')
log.col <- paste('cat',seq(3:26), sep='')
# Split 'dataset
set.seed(8) # The greatest number there ever was
intrain<-createDataPartition(y=df_ML$target,p=0.7,list=FALSE)
df_ML_train<-df_ML[intrain,]
df_ML_validate<-df_ML[-intrain,]
dim(df_ML_train)
dim(df_ML_validate)
library(neuralnet)
#install.packages('fastDummies')
library(fastDummies)
# Make normalized neural network dataset
df_NN = as.data.frame(scale(df_ML[,c(1:59,86)], center=T, scale=T)) # mean and sd scaling
df_NN = cbind(df_NN, df_ML[,60:85])
############################################################################
## Neural Network ## - This is not working
############################################################################
install.packages('neuralnet')
library(neuralnet)
install.packages('fastDummies')
library(fastDummies)
# Make normalized neural network dataset
df_NN = as.data.frame(scale(df_ML[,c(1:59,86)], center=T, scale=T)) # mean and sd scaling
View(df_NN)
df_NN = cbind(df_NN, df_ML[,60:85])
df_NN = dummy_cols(df_NN)
df_NN = df_NN[,c(1:60,87:103)]
View(df_NN)
set.seed(8) # The greatest number there ever was
intrain <- createDataPartition(y=df_ML$target,p=0.7,list=FALSE)
df_NN_train<-df_NN[intrain,]
df_NN_validate<-df_NN[-intrain,]
dim(df_NN_train)
f = as.formula(target~num1+num2+num3+num4+num5+num6+num7+num8+num9+num10+num11+num12+num13+num14+num15+num16+num17+num18+num19+num20+
num21+num22+num23+num24+num25+num26+num27+num28+num29+num30+num31+num32+num33+num34+num35+num36+num37+num38+num39+num40+
num41+num42+num43+num44+num45+num46+num47+num48+num49+num50+num51+num52+num53+num54+num55+num56+num57+num58+num59+
cat1_E+cat1_A+cat1_C+cat1_D+cat1_B+cat2_D+cat2_G+cat2_L+cat2_B+cat2_F+cat2_H+cat2_C+cat2_K+cat2_E+cat2_I+cat2_A+cat2_J)
help("neuralnet")
nnet1 = neuralnet(f, data=df_NN_train, hidden=1)
nnet1
results1 = compute(nnet1, df_NN_validate) # This line is breaking
nnet1$result.list
nnet1$result.matrix
nnet1$model.list$variables
colnames(df_NN_validate)
nnet1$model.list$variables - colnames(df_NN_validate)
nnet1$model.list$variables == colnames(df_NN_validate)
results1 = compute(nnet1, df_NN_validate[-target]) # This line is breaking
results1 = compute(nnet1, df_NN_validate[-"target"]) # This line is breaking
results1 = compute(nnet1, df_NN_validate[-c(target)]) # This line is breaking
results1 = compute(nnet1, df_NN_validate[-c("target")]) # This line is breaking
results1 = compute(nnet1, df_NN_validate[,-c("target")]) # This line is breaking
results1 = compute(nnet1, df_NN_validate[,!(names(df_NN_validate)=="target")]) # This line is breaking
nnet1Pred=results1$net.result
abs(df_NN_validate$target - nnet1Pred)
mean(abs(df_NN_validate$target - nnet1Pred))
View(nnet1)
View(nnet1)
dim(df_NN_train)
nnet1 = neuralnet(f, data=df_NN_train, hidden=3)
nnet1
results1 = compute(nnet1, df_NN_validate[,!(names(df_NN_validate)=="target")]) # This line is breaking
help("neuralnet")
nnet1 = neuralnet(f, data=df_NN_train, hidden=3, threshold = 0.05, stepmax = 3e+05) #run time ~ 4 min start 11:34...
results1 = compute(nnet1, df_NN_validate[,!(names(df_NN_validate)=="target")]) # This line is breaking
nnet1Pred=results1$net.result
# mean absolute error
mean(abs(df_NN_validate$target - nnet1Pred)) #0.63 w/ 1 hidden and threshold at 0.01
nnet1 = neuralnet(f, data=df_NN_train, hidden=3, threshold = 0.01, stepmax = 3e+05) #run time ~ 12 min
nnet1 = neuralnet(f, data=df_NN_train, hidden=1, threshold = 0.01, stepmax = 3e+05) #run time ~ 12 min
results1 = compute(nnet1, df_NN_validate[,!(names(df_NN_validate)=="target")]) # This line is breaking
nnet1Pred=results1$net.result
# mean absolute error
mean(abs(df_NN_validate$target - nnet1Pred)) #0.63 w/ 1 hidden and threshold at 0.01
nnet1 = neuralnet(f, data=df_NN_train, hidden=2, threshold = 0.01, stepmax = 3e+05) #run time ~ 12 min
nnet1
results1 = compute(nnet1, df_NN_validate[,!(names(df_NN_validate)=="target")]) # This line is breaking
nnet1Pred=results1$net.result
warnings()
results1 = compute(nnet1, df_NN_validate[,!(names(df_NN_validate)=="target")]) # This line is breaking
nnet1Pred=results1$net.result
nnet1 = neuralnet(f, data=df_NN_train, hidden=1, threshold = 0.01, stepmax = 3e+05) #run time ~ 12 min
nnet1
results1 = compute(nnet1, df_NN_validate[,!(names(df_NN_validate)=="target")]) # This line is breaking
nnet1Pred=results1$net.result
# mean absolute error
mean(abs(df_NN_validate$target - nnet1Pred))
set.seed(8) # The greatest number there ever was
nnet1 = neuralnet(f, data=df_NN_train, hidden=1, threshold = 0.01, stepmax = 3e+05) #run time will vary based on hidden layers
library(beepr) #needed for beeps
beep(sound = 1)
beep(sound = 4)
beep(sound = 3)
beep(sound = 3)
set.seed(8) # The greatest number there ever was
intrain <- createDataPartition(y=df_ML$target,p=0.7,list=FALSE)
df_NN_train<-df_NN[intrain,]
df_NN_validate<-df_NN[-intrain,]
dim(df_NN_train)
f = as.formula(target~num1+num2+num3+num4+num5+num6+num7+num8+num9+num10+num11+num12+num13+num14+num15+num16+num17+num18+num19+num20+
num21+num22+num23+num24+num25+num26+num27+num28+num29+num30+num31+num32+num33+num34+num35+num36+num37+num38+num39+num40+
num41+num42+num43+num44+num45+num46+num47+num48+num49+num50+num51+num52+num53+num54+num55+num56+num57+num58+num59+
cat1_E+cat1_A+cat1_C+cat1_D+cat1_B+cat2_D+cat2_G+cat2_L+cat2_B+cat2_F+cat2_H+cat2_C+cat2_K+cat2_E+cat2_I+cat2_A+cat2_J)
nnet1 = neuralnet(f, data=df_NN_train, hidden=1, threshold = 0.01, stepmax = 3e+05) #run time will vary based on hidden layers
beep(sound = 3)
nnet1$weights
results1 = compute(nnet1, df_NN_validate[,!(names(df_NN_validate)=="target")]) # This line is breaking
nnet1Pred=results1$net.result
# mean absolute error
mean(abs(df_NN_validate$target - nnet1Pred))
library(Hmisc)
df_NN_vc <- varclus(df_NN)
df_NN_vc <- varclus(df_NN[,c(1:59)])
str(df_NN[,c(1:59))
str(df_NN[,c(1:59)])
is.numeric(df_NN)
View(df_NN)
is.numeric(df_NN[,c(1:59)])
str(df_NN[,c(1:59)])
help(varclus)
df_NN_vc <- varclus(df=df_NN[,c(1:59)])
is.numeric(as.numeric(df_NN[,c(1:59)]))
is.numeric(as.matrix(df_NN[,c(1:59)]))
df_NN_vc <- varclus(as.matrix(df_NN[,c(1:59)]))
View(df_NN_vc)
df_NN_vc[["sim"]]
View(df_NN_vc)
View(df_NN_vc$sim)
plot(df_NN_vc)
rm(df_NN_vc)
NN_vc <- varclus(as.matrix(df_NN[,c(1:59)]))
is.numeric(as.matrix(df_NN[,c(1:59)]))
plot(NN_vc)
NN_vc$hclust
groups <- cutree(NN_vc$hclust, 16)
groups
rect.hclust(res.hc, k = 16, border = 2:5)
rect.hclust(NN_vc, k = 16, border = 2:5)
rect.hclust(NN_vc, k = 16)
rect.hclust(NN_vc$hclust, k = 16)
rect.hclust(NN_vc$hclust, k = 16, border=1:8)
rect.hclust(NN_vc$hclust, k = 16, border=1:3)
rect.hclust(NN_vc$hclust, k = 16, border=2:4)
groups$clusters
GROUPS$
Groups
groups
