testData = testData[,2:86]
testData_scale = scale(testData[,1:59], center=T, scale=T)
testData_scale2 = cbind(testData_scale, testData[,60:85])
View(testData_scale2)
bestgamma=0.5
bestc=1
svm1=svm(target~., data=SVM_trainscale2, kernel='radial', gamma=bestgamma, cost=bestc)
# Predict
test_pred=predict(svm1,testData_scale2)
testData_scale2$cat1 <- as.factor(testData_scale2$cat1)
testData_scale2$cat2 <- as.factor(testData_scale2$cat2)
svm1=svm(target~., data=SVM_trainscale2, kernel='radial', gamma=bestgamma, cost=bestc)
# Predict
test_pred=predict(svm1,testData_scale2)
(testData_scale2$cat1)
distinct(SVM_validscale2)
distinct(SVM_validscale2@cat1)
distinct(testData_scale2$cat1)
distinct(SVM_validscale2$cat1)
testData = testData[,2:86]
testData_scale = scale(testData[,1:59], center=T, scale=T)
testData_scale2 = cbind(testData_scale, testData[,60:85])
distinct(testData_scale2$cat1)
testData = read.csv('C:\\Users\\jlmic\\Documents\\Machine Learning\\Data\\testData.csv')
testData = testData[,2:86]
testData_scale = scale(testData[,1:59], center=T, scale=T)
testData_scale2 = cbind(testData_scale, testData[,60:85])
distinct(testData_scale2$cat1)
distinct(SVM_validscale2$cat1)
class(testData_scale2$cat1)
# Predict on test
testData = read.csv('C:\\Users\\jlmic\\Documents\\Machine Learning\\Data\\testData.csv')
testData = testData[,2:86]
class(testData$cat1)
df_SVM_train<-df_ML[intrain,]
df_SVM_validate<-df_ML[-intrain,]
class(df_SVM_train$cat1)
distinct(df_SVM_train$cat1)
rm(list=ls())
library(tidyverse)
library(caret)
options(digits=2)
df_ML = read.csv('C:\\Users\\jlmic\\Documents\\Machine Learning\\Data\\MLProjectData.csv')
#df_ML = read.csv('C:\\Users\\Steven\\Documents\\MSA\\Analytics Foundations\\Machine Learning\\data\\MLProjectData.csv')
#df_ML= read.csv('C:\\Users\\chels\\Desktop\\MSA\\Fall 3\\Machine Learning\\Project\\MLProjectData.csv')
# column groups
cat.col = c('cat1','cat2','cat3','cat4','cat5','cat6','cat7','cat8','cat9','cat10','cat11','cat12','cat13','cat14','cat15',
'cat16','cat17','cat18','cat19','cat20','cat21','cat22','cat23','cat24','cat25','cat26')
num.col <- paste('num',seq(1:59), sep='')
log.col <- paste('cat',seq(3:26), sep='')
# Split 'Training' dataset
set.seed(8) # The greatest number there ever was
intrain<-createDataPartition(y=df_ML$target,p=0.7,list=FALSE)
df_ML_train<-df_ML[intrain,]
df_ML_validate<-df_ML[-intrain,]
dim(df_ML_train)
dim(df_ML_validate)
rm(list=ls())
library(tidyverse)
library(caret)
options(digits=2)
df_ML = read.csv('C:\\Users\\jlmic\\Documents\\Machine Learning\\Data\\MLProjectData.csv')
#df_ML = read.csv('C:\\Users\\Steven\\Documents\\MSA\\Analytics Foundations\\Machine Learning\\data\\MLProjectData.csv')
#df_ML= read.csv('C:\\Users\\chels\\Desktop\\MSA\\Fall 3\\Machine Learning\\Project\\MLProjectData.csv')
# column groups
cat.col = c('cat1','cat2','cat3','cat4','cat5','cat6','cat7','cat8','cat9','cat10','cat11','cat12','cat13','cat14','cat15',
'cat16','cat17','cat18','cat19','cat20','cat21','cat22','cat23','cat24','cat25','cat26')
num.col <- paste('num',seq(1:59), sep='')
log.col <- paste('cat',seq(3:26), sep='')
# Split 'Training' dataset
set.seed(8) # The greatest number there ever was
intrain<-createDataPartition(y=df_ML$target,p=0.7,list=FALSE)
df_ML_train<-df_ML[intrain,]
df_ML_validate<-df_ML[-intrain,]
dim(df_ML_train)
dim(df_ML_validate)
library(randomForest)
library(ModelMetrics)
library('Matrix')
#install.packages('xgboost')
library(xgboost)
library(readr)
library(stringr)
library(caret)
library(car)
df_SVM_train<-df_ML[intrain,]
df_SVM_validate<-df_ML[-intrain,]
distinct(df_SVM_train$cat1)
unique(df_SVM_validate$cat1)
unique(SVM_validscale2$cat1)
SVM_trainscale = scale(df_SVM_train[,1:59], center=T, scale=T)
SVM_trainscale2 = cbind(SVM_trainscale, df_SVM_train[,60:86])
SVM_validscale = scale(df_SVM_validate[,1:59], center=T, scale=T)
SVM_validscale2 = cbind(SVM_validscale, df_SVM_validate[,60:86])
#install.packages("e1071")
library(e1071)
###########################################################################
###########################################################################
############     TUNE THE SVM HYPERPARAMETERS GAMMA AND C - Takes an eternity     ############
###########################################################################
###########################################################################
# TuneSVM = tune.svm(target~., data=SVM_trainscale, kernel='radial', gamma=2^(-5:-.5), cost=2^(-1:3))
# summary(TuneSVM)
# plot(TuneSVM)
# - Detailed performance results:
#   gamma cost error dispersion
# 1  0.031  0.5   2.2       0.36
# 2  0.062  0.5   2.2       0.36
# 3  0.125  0.5   2.1       0.35
# 4  0.250  0.5   2.1       0.35
# 5  0.500  0.5   2.1       0.35
# 6  0.031  1.0   2.2       0.37
# 7  0.062  1.0   2.2       0.36
# 8  0.125  1.0   2.1       0.35
# 9  0.250  1.0   2.1       0.35
# 10 0.500  1.0   2.1       0.35
# 11 0.031  2.0   2.3       0.38
# 12 0.062  2.0   2.3       0.37
# 13 0.125  2.0   2.2       0.35
# 14 0.250  2.0   2.1       0.35
# 15 0.500  2.0   2.1       0.35
# 16 0.031  4.0   2.4       0.39
# 17 0.062  4.0   2.3       0.38
# 18 0.125  4.0   2.2       0.36
# 19 0.250  4.0   2.1       0.35
# 20 0.500  4.0   2.1       0.36
# 21 0.031  8.0   2.5       0.40
# 22 0.062  8.0   2.3       0.38
# 23 0.125  8.0   2.2       0.36
# 24 0.250  8.0   2.1       0.35
# 25 0.500  8.0   2.1       0.36
############################################################################
############################################################################
## USE THE BEST HYPERPARAMETERS TO BUILD FINAL MODEL AND TEST PERFORMANCE ##
############################################################################
############################################################################
#### Final SVM Model - MAE = 0.34/0.93 ####
bestgamma=0.5
bestc=1
svm1=svm(target~., data=SVM_trainscale2, kernel='radial', gamma=bestgamma, cost=bestc)
# Predict
pred=predict(svm1,SVM_trainscale2)
pred2=predict(svm1,SVM_validscale2)
# MAE
df_SVM_train$Prediction = pred
df_SVM_train$abserror = abs(df_SVM_train$target - df_SVM_train$Prediction)
MAE = mean(df_SVM_train$abserror)
print(MAE)
df_SVM_validate$Prediction = pred2
df_SVM_validate$abserror = abs(df_SVM_validate$target - df_SVM_validate$Prediction)
MAE = mean(df_SVM_validate$abserror)
print(MAE)
testData = read.csv('C:\\Users\\jlmic\\Documents\\Machine Learning\\Data\\testData.csv')
testData = testData[,2:86]
testData_scale = scale(testData[,1:59], center=T, scale=T)
testData_scale2 = cbind(testData_scale, testData[,60:85])
unique(testData_scale2$cat1)
unique(SVM_validscale2$cat1)
unique(testData_scale2$cat2)
unique(SVM_validscale2$cat2)
unique(testData_scale2$cat3)
unique(SVM_validscale2$cat3)
unique(testData_scale2$cat4)
unique(SVM_validscale2$cat4)
unique(testData_scale2$cat5)
unique(SVM_validscale2$cat5)
test_pred=predict(svm1,testData_scale2) # NO clue why this is breaking. I've tried changing values to factors.
unique(testData_scale2$cat6)
unique(SVM_validscale2$cat6)
unique(testData_scale2$cat7)
unique(SVM_validscale2$cat7)
unique(testData_scale2$cat8)
unique(SVM_validscale2$cat8)
unique(testData_scale2$cat9)
unique(SVM_validscale2$cat9)
unique(testData_scale2$cat10)
unique(SVM_validscale2$cat10)
unique(testData_scale2$cat11)
unique(SVM_validscale2$cat11)
unique(testData_scale2$cat12)
unique(SVM_validscale2$cat12)
unique(testData_scale2$cat13)
unique(SVM_validscale2$cat13)
unique(testData_scale2$cat14)
unique(SVM_validscale2$cat14)
unique(testData_scale2$cat15)
unique(SVM_validscale2$cat15)
unique(testData_scale2$cat16)
unique(SVM_validscale2$cat16)
unique(testData_scale2$cat17)
unique(SVM_validscale2$cat17)
unique(testData_scale2$cat18)
unique(SVM_validscale2$cat18)
unique(testData_scale2$cat19)
unique(SVM_validscale2$cat19)
unique(testData_scale2$cat20)
unique(SVM_validscale2$cat20)
unique(testData_scale2$cat21)
unique(SVM_validscale2$cat21)
unique(testData_scale2$cat22)
unique(SVM_validscale2$cat22)
unique(testData_scale2$cat23)
unique(SVM_validscale2$cat23)
unique(testData_scale2$cat24)
unique(SVM_validscale2$cat24)
unique(testData_scale2$cat25)
unique(SVM_validscale2$cat25)
unique(testData_scale2$cat26)
unique(SVM_validscale2$cat26)
unique(testData_scale2$cat27)
accuracy_xgb=vector()
eta=seq(.05,1,by=.05)
depth = seq(1,15, by=1)
i=1
for (d in depth) {
for (e in eta) {
print(i)
xgb <- xgboost(data = sparse_train,
label = train_label,
eta = 0,
max_depth = 20,
gamma = 0,
nround=100,
subsample = 0.75,
colsample_bytree = 0.75,
objective = "reg:linear",
nthread = 3,
eval_metric = 'rmse',
verbose =0)
ptrain = predict(xgb, sparse_train)
pvalid = predict(xgb, sparse_valid)
abserror = abs(df_XG_validate$target - pvalid)
accuracy_xgb[i] = mean(abserror)
i=i+1
}
}
plot(eta, accuracy_xgb)
plot(depth, accuracy_xgb)
# Tune for eta
accuracy_xgb=vector()
eta=seq(.05,1,by=.05)
accuracy_xgb=vector()
eta=seq(.05,1,by=.05)
depth = seq(1,15, by=1)
i=1
for (d in depth) {
for (e in eta) {
print(i)
xgb <- xgboost(data = sparse_train,
label = train_label,
eta = 0,
max_depth = d,
gamma = 0,
nround=100,
subsample = 0.75,
colsample_bytree = 0.75,
objective = "reg:linear",
nthread = 3,
eval_metric = 'rmse',
verbose =0)
ptrain = predict(xgb, sparse_train)
pvalid = predict(xgb, sparse_valid)
abserror = abs(df_XG_validate$target - pvalid)
accuracy_xgb[i] = mean(abserror)
i=i+1
}
}
df_XG_train<-df_ML[intrain,]
df_XG_validate<-df_ML[-intrain,]
##############################################################################################################
#Changing all categorical vars to factors
df_XG_train[cat.col] = lapply(df_XG_train[cat.col], factor)
df_XG_validate[cat.col] = lapply(df_XG_validate[cat.col], factor)
sparse_train = sparse.model.matrix(target ~ . -target , data=df_XG_train)
sparse_valid = sparse.model.matrix(target  ~ . -target , data=df_XG_validate)
train_label = as.numeric(df_XG_train$target)[df_XG_train$target]
accuracy_xgb=vector()
eta=seq(.05,1,by=.05)
depth = seq(1,15, by=1)
i=1
for (d in depth) {
for (e in eta) {
print(i)
xgb <- xgboost(data = sparse_train,
label = train_label,
eta = 0,
max_depth = d,
gamma = 0,
nround=100,
subsample = 0.75,
colsample_bytree = 0.75,
objective = "reg:linear",
nthread = 3,
eval_metric = 'rmse',
verbose =0)
ptrain = predict(xgb, sparse_train)
pvalid = predict(xgb, sparse_valid)
abserror = abs(df_XG_validate$target - pvalid)
accuracy_xgb[i] = mean(abserror)
i=i+1
}
}
plot(eta, accuracy_xgb)
plot(eta, accuracy_xgb)
View(accuracy_xgb)
accuracy_xgb=vector()
eta=seq(.05,1,by=.05)
i=1
for (e in eta) {
print(i)
xgb <- xgboost(data = sparse_train,
label = train_label,
eta = e,
max_depth = 20,
gamma = 0,
nround=100,
subsample = 0.75,
colsample_bytree = 0.75,
objective = "reg:linear",
nthread = 3,
eval_metric = 'rmse',
verbose =0)
ptrain = predict(xgb, sparse_train)
pvalid = predict(xgb, sparse_valid)
abserror = abs(df_XG_validate$target - pvalid)
accuracy_xgb[i] = mean(abserror)
i=i+1
}
plot(eta, accuracy_xgb)
accuracy=vector()
mtry=seq(2,86,by=2)
i=1
for(m in mtry){
print(i)
rf = randomForest(target ~ ., data=df_ML_train,mtry=m, ntree=50)
test_pred =  predict(rf,df_ML_validate)
#accuracy[i] =  sum(test_pred!=df_ML_train$target)/nrow(df_ML_train)
abserror = abs(df_ML_validate$target - test_pred)
accuracy[i] = mean(abserror)
i=i+1
}
plot(mtry, accuracy)
accuracy=vector()
ntree=seq(25,200,by=25)
i=1
for(n in ntree){
print(i)
rf = randomForest(target ~ ., data=df_ML_train,mtry=22, ntree=n)
test_pred =  predict(rf,df_ML_validate)
#accuracy[i] =  sum(test_pred!=df_ML_train$target)/nrow(df_ML_train)
abserror = abs(df_ML_validate$target - test_pred)
accuracy[i] = mean(abserror)
i=i+1
}
plot(ntree, accuracy)
accuracy_xgb=vector()
eta=seq(.05,1,by=.05)
i=1
for (e in eta) {
print(i)
xgb <- xgboost(data = sparse_train,
label = train_label,
eta = e,
max_depth = 20,
gamma = 0,
nround=100,
subsample = 0.75,
colsample_bytree = 0.75,
objective = "reg:linear",
nthread = 3,
eval_metric = 'rmse',
verbose =0)
ptrain = predict(xgb, sparse_train)
pvalid = predict(xgb, sparse_valid)
abserror = abs(df_XG_validate$target - pvalid)
accuracy_xgb[i] = mean(abserror)
i=i+1
}
plot(eta, accuracy_xgb)
accuracy_xgb=vector()
depth = seq(1,15, by=1)
i=1
for (d in depth) {
print(i)
xgb <- xgboost(data = sparse_train,
label = train_label,
eta = 0.5,
max_depth = d,
gamma = 0,
nround=100,
subsample = 0.75,
colsample_bytree = 0.75,
objective = "reg:linear",
nthread = 3,
eval_metric = 'rmse',
verbose =0)
ptrain = predict(xgb, sparse_train)
pvalid = predict(xgb, sparse_valid)
abserror = abs(df_XG_validate$target - pvalid)
accuracy_xgb[i] = mean(abserror)
i=i+1
}
plot(depth, accuracy_xgb)
accuracy_xgb=vector()
gam = seq(0,1, by=.1)
i=1
for (g in gam) {
print(i)
xgb <- xgboost(data = sparse_train,
label = train_label,
eta = 0.5,
max_depth = 5,
gamma = g,
nround=100,
subsample = 0.75,
colsample_bytree = 0.75,
objective = "reg:linear",
nthread = 3,
eval_metric = 'rmse',
verbose =0)
ptrain = predict(xgb, sparse_train)
pvalid = predict(xgb, sparse_valid)
abserror = abs(df_XG_validate$target - pvalid)
accuracy_xgb[i] = mean(abserror)
i=i+1
}
plot(gam, accuracy_xgb)
xgb <- xgboost(data = sparse_train,
label = train_label,
eta = 0.05,
max_depth = 5,
gamma = 0.6,
nround=100,
subsample = 0.75,
colsample_bytree = 0.75,
objective = "reg:linear",
nthread = 3,
eval_metric = 'rmse',
verbose =0)
# Predict
ptrain = predict(xgb, sparse_train)
pvalid = predict(xgb, sparse_valid)
# MAE
df_XG_train$Prediction = ptrain
df_XG_train$abserror = abs(df_XG_train$target - df_XG_train$Prediction)
MAE = mean(df_XG_train$abserror)
print(MAE)
df_XG_validate$Prediction = pvalid
df_XG_validate$abserror = abs(df_XG_validate$target - df_XG_validate$Prediction)
MAE = mean(df_XG_validate$abserror)
print(MAE)
xgb <- xgboost(data = sparse_train,
label = train_label,
eta = 0.05,
max_depth = 10,
gamma = 0.1,
nround=100,
subsample = 0.75,
colsample_bytree = 0.75,
objective = "reg:linear",
nthread = 3,
eval_metric = 'rmse',
verbose =0)
# Predict
ptrain = predict(xgb, sparse_train)
pvalid = predict(xgb, sparse_valid)
# MAE
df_XG_train$Prediction = ptrain
df_XG_train$abserror = abs(df_XG_train$target - df_XG_train$Prediction)
MAE = mean(df_XG_train$abserror)
print(MAE)
df_XG_validate$Prediction = pvalid
df_XG_validate$abserror = abs(df_XG_validate$target - df_XG_validate$Prediction)
MAE = mean(df_XG_validate$abserror)
print(MAE)
xgb <- xgboost(data = sparse_train,
label = train_label,
eta = 0.05,
max_depth = 5,
gamma = 0.1,
nround=100,
subsample = 0.75,
colsample_bytree = 0.75,
objective = "reg:linear",
nthread = 3,
eval_metric = 'rmse',
verbose =0)
# Predict
ptrain = predict(xgb, sparse_train)
pvalid = predict(xgb, sparse_valid)
# MAE
df_XG_train$Prediction = ptrain
df_XG_train$abserror = abs(df_XG_train$target - df_XG_train$Prediction)
MAE = mean(df_XG_train$abserror)
print(MAE)
df_XG_validate$Prediction = pvalid
df_XG_validate$abserror = abs(df_XG_validate$target - df_XG_validate$Prediction)
MAE = mean(df_XG_validate$abserror)
print(MAE)
# Predict
test_pred=predict(svm1,testData_scale2) # NO clue why this is breaking.
# Predict on test
testData = read.csv('C:\\Users\\jlmic\\Documents\\Machine Learning\\Data\\testData.csv')
dim(testData)
df_ML = read.csv('C:\\Users\\jlmic\\Documents\\Machine Learning\\Data\\MLProjectData.csv')
#df_ML = read.csv('C:\\Users\\Steven\\Documents\\MSA\\Analytics Foundations\\Machine Learning\\data\\MLProjectData.csv')
#df_ML= read.csv('C:\\Users\\chels\\Desktop\\MSA\\Fall 3\\Machine Learning\\Project\\MLProjectData.csv')
dim(df_ML
#df_ML = read.csv('C:\\Users\\Steven\\Documents\\MSA\\Analytics Foundations\\Machine Learning\\data\\MLProjectData.csv')
#df_ML= read.csv('C:\\Users\\chels\\Desktop\\MSA\\Fall 3\\Machine Learning\\Project\\MLProjectData.csv')
dim(df_ML)
View(testData)
